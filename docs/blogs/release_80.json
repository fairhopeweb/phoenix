{
  "Title": "How we made our optical character recognition shine for code",
  "Slug": "how-we-made-our-optical-character-recognition-shine-for-code",
  "Collection ID": "642da2c43fe34c364a69857f",
  "Item ID": "642da2c43fe34c8c546989f5",
  "Created On": "Thu Jun 02 2022 20:39:55 GMT+0000 (Coordinated Universal Time)",
  "Updated On": "Fri Mar 31 2023 18:30:40 GMT+0000 (Coordinated Universal Time)",
  "PublishedOn": "Wed Apr 05 2023 16:53:15 GMT+0000 (Coordinated Universal Time)",
  "MainImage": "https://uploads-ssl.webflow.com/6143afec68f555387049efb3/62ed63aba4f21f0c13ac0b09_ocr.png",
  "Post summary": "We've spent months developing an OCR engine that's fine-tuned to code through a combination of pre- and post-processing.",
  "FullPost": "<p id=\"\">For the past year, I’ve been a member of a team of machine learning engineers at <a href=\"http://pieces.app/\" target=\"_blank\" id=\"\">Pieces.app</a> working to develop an optical character recognition (OCR) engine specifically for code.</p><p id=\"\">Our OCR engine at Pieces.app uses <a href=\"https://github.com/tesseract-ocr/tesseract\" target=\"_blank\" id=\"\">Tesseract</a> as its main OCR engine. Tesseract performs a layout analysis and then uses an LSTM trained on text-image pairs to predict the characters in the image. It is currently one of the best free OCR tools and supports over 100 languages, but is not particularly fine-tuned to code. To make it really shine for code snippets, we added specific pre and post-processing steps to our recognition pipeline.</p><h2 id=\"\">Standardized inputs through image pre-processing</h2><p id=\"\">To best support software engineers when they want to transcribe code from images, we fine-tuned our pre-processing pipeline to screenshots of code in IDEs, terminals and online resources like YouTube videos and blog posts. Since programming environments can be in light or dark mode, both modes should yield good results. Additionally, we wanted to support images with gradients or noisy backgrounds, as might be found in YouTube programming tutorials or retro websites, as well as images with low resolution, for example from being compressed from uploading or sending a screenshot.</p><p id=\"\">‍</p><figure id=\"\" class=\"w-richtext-figure-type- w-richtext-align-center\" data-rt-type=\"\" data-rt-align=\"center\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/6143afec68f555387049efb3/62991fd5cb7f76dd1933f035_1*7nuWo8H5TtpK-QMOBuTBLw.png\" width=\"auto\" height=\"auto\" loading=\"auto\" id=\"\"></div></figure><figure id=\"\" class=\"w-richtext-figure-type- w-richtext-align-center\" data-rt-type=\"\" data-rt-align=\"center\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/6143afec68f555387049efb3/62991fd568fb6b9c0f679cd5_1*T721ZoLSMh4QP-IyXdMMsQ.png\" width=\"auto\" height=\"auto\" loading=\"auto\" id=\"\"></div><figcaption id=\"\">Dark and light mode input images</figcaption></figure><p id=\"\">Since Tesseract works best on binarized, light-mode images, we needed to invert dark-mode images in pre-processing. To determine which images are in dark mode, our engine first median-blurs the image to remove outliers, and then calculates the average pixel brightness. If it is lower than a specific threshold, it is determined to be dark and thus inverted. To handle gradient and noisy backgrounds, we use a dilation-based approach. We generate a copy of the image and apply a dilation kernel and median blur on it. We then subtract this blurred copy from the original image to remove dark areas without disturbing the text on the image. For<strong id=\"\"> </strong>low-resolution images, we upsample the image depending on the input size using bicubic upsampling.</p><figure id=\"\" class=\"w-richtext-figure-type- w-richtext-align-center\" data-rt-type=\"\" data-rt-align=\"center\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/6143afec68f555387049efb3/62991fd51e03982cdc1b2f4a_1*moRf8wICgZdMippTt-qWdg.png\" width=\"auto\" height=\"auto\" loading=\"auto\" id=\"\"></div></figure><figure id=\"\" class=\"w-richtext-figure-type- w-richtext-align-center\" data-rt-type=\"\" data-rt-align=\"center\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/6143afec68f555387049efb3/62991fd5252a365dbcb0a30b_1*VyoiQpQec29ccAGeYlVweg.png\" width=\"auto\" height=\"auto\" loading=\"auto\" id=\"\"></div><figcaption id=\"\">Inverted and binarized input image</figcaption></figure><h2 id=\"\">Code requires layout formatting</h2><p id=\"\">On the text prediction of Tesseract, we perform a layout analysis and infer the indentation of the produced code. Tesseract, by default, does not indent any output, which can not only make code less readable, but even change its meaning in languages such as Python. To add indentation, we use the bounding boxes that Tesseract returns for every line of code. Using the width of the box and the number of characters found in it, we calculate the average width of a character in that line. We then use the starting coordinates of the box to calculate by how many spaces is it indented compared to the other code lines. After that, we use a simple heuristic to push the indentations into even numbers of spaces.</p><figure id=\"\" class=\"w-richtext-figure-type- w-richtext-align-center\" data-rt-type=\"\" data-rt-align=\"center\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/6143afec68f555387049efb3/62991fd57414180cf8263f4d_1*j2s9JsjOphtkKmZtvhdtuw.png\" width=\"auto\" height=\"auto\" loading=\"auto\" id=\"\"></div></figure><figure id=\"\" class=\"w-richtext-figure-type- w-richtext-align-center\" data-rt-type=\"\" data-rt-align=\"center\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/6143afec68f555387049efb3/62991fd5522f3215d2a3e2cd_1*J6R7i54ML4bHD9ZASvDb7g.png\" width=\"auto\" height=\"auto\" loading=\"auto\" id=\"\"></div><figcaption id=\"\">Transcribed code with and without inferred indentation</figcaption></figure><h2 id=\"\">Evaluating our pipeline</h2><p id=\"\">To evaluate our modifications to the OCR pipeline, we use multiple sets of hand-crafted and generated datasets of image-text pairs. By running OCR on each image, we then calculate the Levenshtein distance between the predicted text and the ground truth. We treat each modification as a research hypothesis and then use experiments to validate them. For upsampling small images, for example, our research hypothesis was that super-resolution models like SRCNN (Super-Resolution Convolutional Neural Network) would boost OCR performance more than standard upsampling methods like nearest-neighbor interpolation or bicubic interpolation. To test this hypothesis, we ran the OCR pipeline multiple times on the same datasets, each time using a different upsampling method. While we found that nearest-neighbor upsampled images yield worse results, we did not find a significant difference between super-resolution based upsampling and bicubic upsampling for our pipeline. Given that super-resolution models need more storage space and have a higher latency than bicubic upsampling, we decided to go with bicubic upsampling for our pipeline.</p><figure id=\"\" class=\"w-richtext-figure-type- w-richtext-align-center\" data-rt-type=\"\" data-rt-align=\"center\"><div id=\"\"><img src=\"https://uploads-ssl.webflow.com/6143afec68f555387049efb3/62991fd53332217499c49f17_1*oL8pt2k95H0UtH9XW5TKPQ.png\" width=\"auto\" height=\"auto\" loading=\"auto\" id=\"\"></div><figcaption id=\"\">Samples from generated text image pairs</figcaption></figure><p id=\"\">Overall, code is a challenging objective for OCR, since it has to capture highly structured syntax and formatting, while allowing for unstructured variable names and code comments. We’re happy to provide one of the first OCR models fine-tuned to code and are continuing to improve the model to make it faster and more accurate, so you can get usable code from your screenshots and continue coding.</p><p id=\"\">To test our model on your own code screenshots, visit <a href=\"http://codefromscreenshot.com/\" target=\"_blank\" id=\"\">codefromscreenshot.com</a> or download the <a href=\"https://code.pieces.app/\" target=\"_blank\" id=\"\">Pieces app</a>. &nbsp;If you’re a developer interested in our APIs, check out <a href=\"http://runtime.dev\" target=\"_blank\" id=\"\">runtime.dev</a> or email us at support@runtime.dev.</p>",
  "Date": "Thu Jul 07 2022 00:00:00 GMT+0000 (Coordinated Universal Time)",
  "Author": "Leonie Bossemeyer",
  "Author Photo": "https://uploads-ssl.webflow.com/6143afec68f555387049efb3/629a4bf78b3a7a0e19b889e9_leonie.jpeg",
  "AMP URL": "",
  "TOC based on...": "h5,h4",
  "Tags": "machine-learning; ocr; programming",
  "CTA": "",
  "CTA link": "",
  "Canonical URL": ""
}
